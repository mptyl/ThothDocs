# Installazione di ThothAI - attività preliminari
**ThothAI** deve essere deployato come container Docker, per garantire isolamento e gestibilità nell'ambiente di produzione.
Gli unici prerequisiti sono la disponibilità di `Python 3.13` o superiore e la disponibilità di Docker e del client `docker-compose`.

!!! note "Installazione di Python e di Docket"

    Le procedure di installazione di Python sono al di fuori dell'ambito di questo documento.
    Andare sul sito di [Python](https://www.python.org/){:target="_blank"} per acquisire le informazioni necessarie a installare o aggiornare Python, se non è già stato fatto.

    Allo stesso modo, l'installazione di Docker e del client `docker-compose` sono al di fuori dell'ambito di questo documento.
    Andare sul sito di [Docker](https://www.docker.com/){:target="_blank"} per acquisire le informazioni necessarie a installare Docker, se non già disponibile.
    

## 1 - Attività preliminare - clonazione del repository

L'attività di installazione richiede prima di tutto l'acquisizione dei sorgenti del progetto. 
**ThothAI** è composto da un singolo repository che contiene sia il backend che il frontend.

Da linea di comando posizionarsi dove si preferisce e clonare il repository:

```bash
git clone https://github.com/mptyl/Thoth.git ThothAI
cd ThothAI
```

Dopo la clonazione, la struttura del progetto sarà simile a questa:

```
ThothAI/
├── backend/                # Applicazione Django (backend)
├── frontend/               # Applicazione Next.js (frontend)
├── docker/                 # File di configurazione Docker
├── scripts/                # Script di installazione e configurazione
├── config.yml              # Template di configurazione
├── .env.local.template     # Template delle variabili d'ambiente
└── ...
```

## 2 - Attività preliminare - configurazione di base

### 2.1 - Preparazione del file di configurazione {#preparazione-configurazione}

Il primo passo **obbligatorio** per l'installazione di ThothAI è creare il file di configurazione personale partendo dal template fornito:

```bash
cp config.yml config.yml.local
```

Il file `config.yml.local` contiene tutte le impostazioni necessarie per il funzionamento di ThothAI. Dovrete configurare:

- **Provider AI**: Attivare almeno un provider LLM, scegliendone uno o più tra OpenAI, Anthropic, Gemini, Mistral, DeepSeek, OpenRouter, Ollama e LMStudio. Si consiglia di configurare come minimo OpenRouter per poter utilizzare il workspace di default che è impostato per utilizzare modelli forniti da OpenRouter. In alternativa si possonono scegliete i provider diretti (OpenAI, Anthropic, ecc), ma, se si vuole usare il workspace demo, devono essere configurati i Model e gli Agent che uttilizzano i provider configurati;
- **Servizio di embedding**: Configurare il servizio per l'embedding utilizzato da Qdrant e dalla gestione LSH. Scegliere tra l'embedding fornito da OpenAI, Mistral, o Cohere;
- **Modello base di backend**: Scegliere il modello base per le attività gestite dal backend, come  la generazione delle descrizioni di tabelle e colonne, la documentazione del database, la creazione dello schema ERD, ecc. Indicare il provider ed il modello;
- **Database utilizzati**: Indicare i database che si intende utilizzare. E' possibile gestire database di tipo SQLite, MariaDB, MySql, PostgreSQL e SQL Server;
- **Credenziali admin**: Impostare username/password per l'accesso amministrativo dell'utente admin, creato di default quando viene eseguito il comando `docker-compose up -d`;
- **Credenziali demo**: Impostare username/password per l'accesso dell'utente demo utilizzabile immediatamente per provare il software senza dover configurare nulla;
- **Monitoraggio**: Configurare Logfire per il monitoraggio dell'applicazione (vedi [monitoring](../4-user_manual/4.3-logging/4.3.1-logfire.md){:target="_blank"}, inddicando se si intende utilizzare il servizio di monitoraggio tramite logfire e, in quel caso, il token di accesso;
- **Porte dei servizi**: Configurare le porte per i vari componenti - se quelle di default sono in conflitto con servizi già in attività sulle stesse porte;
- **Docker Configuration**: Configurare le opzioni di Docker - se si intende utilizzare un network Docker personalizzato, specificare il nome del network;
- **Runtime**: Configurare le opzioni di runtime come le modalità di lavoro di Django (debug o no) e i livelli di log distinti tra frontend e backend.


!!! note "Provider LLM consigliato per iniziare"

    Per una rapida prova di ThothAI, si consiglia di utilizzare **OpenRouter** che fornisce accesso a centinaia di modelli LLM con una singola API key e include anche modelli gratuiti. L'installazione Docker di ThothAI prevede lla creazione di un ambiente demo già preconfigurato per utilizzare OpenRouter come provider e CaliforniaSchools come database su cui esercitarsi.

#### 2.1.1 - Parametri correnti dal file `config.yml`

Di seguito i parametri attualmente impostati nel file di configurazione principale `config.yml` alla radice del progetto (`ThothAI/config.yml`). Usateli come riferimento durante la preparazione di `config.yml.local`.

```yaml
version: "1.0"

ai_providers:
  openai:
    enabled: false
    api_key: ""
  anthropic:
    enabled: false
    api_key: ""
  gemini:
    enabled: false
    api_key: ""
  mistral:
    enabled: false
    api_key: ""
  deepseek:
    enabled: false
    api_key: ""
    api_base: "https://api.deepseek.com/v1"
  openrouter:
    enabled: true
    api_key: ""
    api_base: "https://openrouter.ai/api/v1"
  ollama:
    enabled: false
    api_base: "http://127.0.0.1:11434"
  lm_studio:
    enabled: false
    api_base: "http://localhost:1234"
  groq:
    enabled: false
    api_key: ""

embedding:
  provider: "openai"
  api_key: ""
  model: "text-embedding-3-small"

backend_ai_model:
  ai_provider: "openrouter"
  ai_model: "google/gemini-2.5-flash"

databases:
  sqlite: true
  postgresql: true
  mysql: false
  mariadb: true
  sqlserver: true

admin:
  email: ""
  username: "admin"
  password: "admin123"

demo:
  email: ""
  username: "demo"
  password: "demo1234"

monitoring:
  enabled: true
  logfire_token: ""

ports:
  frontend: 3040
  backend: 8000
  sql_generator: 8020
  nginx: 8040
  qdrant: 6333

docker:
  network_name: "thoth-network"
  compose_file: "docker-compose.yml"
  build_cache: true

runtime:
  debug: false
  backend_log_level: "WARNING"
  frontend_log_level: "WARNING"

relevance_guard:
  strict_min_score: 0.75
  weak_min_score: 0.45
  drop_below: 0.30
  w_bm25: 0.60
  w_struct: 0.40
  use_rrf: false
  strict_fails_required: 2
```

Nota: i campi `api_key` sono volutamente vuoti nel file di esempio. Inserite le vostre chiavi dove appropriato oppure utilizzate il file `.env.local` in modalità sviluppo locale.

## 3 - Attività preliminare - acquisizione delle API Keys

Prima di procedere con l'installazione vera e propria, è necessario dotarsi delle API Keys per:

### 3.1 - I servizi di generazione tramite LLM

**ThothAI** è predisposto per potersi collegare ai seguenti fornitori di API LLM:

* OpenAI 
* Gemini (di Google)
* Anthropic 
* Mistral
* DeepSeek
* OpenRouter (che fa da proxy a centinaia di provider)
* LMStudio (se non in locale)
* Ollama (se non in locale)


Ogni fornitore di API, a sua volta, permette di usare diversi modelli, che andranno specificati nel setup dell'applicazione.

In fase di configurazione occorre stabilire quali provider si desidera utilizzare. Per ognuno di essi è necessario procurarsi una API Key.
Se si utilizza Ollama o LMStudio in locale, ovviamente, non è necessaria alcuna API Key, ma solo indicare l'IP del server utilizzato se diverso da quello standard. 

Per procurarsi l'API Key occorre seguire le procedure previste dal provider.

!!! note "Assegnazione di una API Key"

    Le procedure di assegnazione di una API Key sono più o meno le stesse per ogni fornitore. 
    Creare delle API Key con il nome che preferite, come ad esempio "ThothKey" o simile, per ogni fornitore che si desidera utilizzare. IQueste chiavi devono essere indicate nel file di configurazione `config.yml` se si vuole utilizzare Docker, altrimenti nel file di configurazione .env.local

Se si usa OpenRouter, con una sola API Key si potrà accedere a una lunga lista di LLM, aventi dimensioni e costi molto variabili (alcuni modelli sono resi disponibili addirittura gratuitamente). Andare sul sito di [OpenRouter](https://openrouter.ai/){:target="_blank"} per ulteriori dettagli. 

Tramite `OpenRouter` si possono utilizzare modelli di provider non previsti dalla lista precedente, in quanto OpenRouter provvede a esporre sempre la stessa interfaccia, compatibile con l'interfaccia di OpenAI, utilizzabile per tutti i modelli che supporta.  

I valori di default impostati in ThothAI prevedono che tutti gli Agent che svolgono le varie fasi del processo usino un modello fornito da OpenRouter. 

L'utente può ovviamente cambiare questo setup (si vedrà più avanti come), ma, se si vuole provare rapidamente ThothAI, l'ideale è iniziare con l'acquisto di 5 o 10 dollari di servizi OpenRouter e lasciare le impostazioni di default che vengono definite in fase di installazione. 

Dai nostri test, utilizzando la configurazione di default che prevede un largo uso di modelli economici nelle fasi meno critiche del processo, ogni esecuzione ha un costo variabile tra 2 e 5 centesimi di dollaro.
Per cui, con una decina di dollari, si possono fare da 200 a 500 esecuzioni di generazione di un SQL a partire da una domanda in linguaggio naturale.


!!! note "Costi di generazione di un SQL"
    Ovviamente i costi di generazione di un SQL sono in funzione del numero di agenti che si vuole attivare in parallelo per la generazione degli SQL e dei Test di verifica. Più agenti, più probabilità di avere un buon SQL, più costo. Allo stesso modo più grandi e potenti sono i modelli utilizzati, più qualità e velocità nella generazione, più costo.


### 3.2 - Il servizio Logfire

[Logfire](https://pydantic.dev/logfire){:target="_blank"} è un servizio messo a disposizione da Pydantic per tracciare l'attività di applicazioni Python. Fino a 10 milioni di messaggi al mese è un servizio gratuito.

ThothAI produce un log delle sue attività contemporaneamente su file, su console e su Logfire. Il log su Logfire persiste per circa 30 giorni e fornisce un elevato livello di dettaglio sulle operazioni registrate.

Su Logfire non vengono mai inviati dati sul contenuto dei database interrogati, ma, se non si vuole utilizzare il servizio, basta lasciare vuota la chiave e Logfire non verrà utilizzato. In questo caso rimarranno solo i file di log salvati in locale [(vedi logs)](../4-user_manual/4.3-logging/4.3.2-log_management.md){:target="_blank"}. Non si avrà però lo stesso livello di granularità e di insight fornita da Logfire.

Se si vuole usare Logfire è necessario quindi andare su [Logfire](https://pydantic.dev/logfire) e crearsi una API Key. Scegliere un server europeo se si sta operando da un paese europeo per avere migliori performance. Salvare l'URL del server per le future consultazioni. 

!!! note "Il logging di un LLM"

    Loggare le attività effettuate da un LLM è un'attività piuttosto complessa. Con Logfire si può facilmente verificare il testo del prompt ricevuto dal LLM, l'eventuale attivazione di tool da parte dell'Agent in esecuzione e l'output generato, con abbondanza di dettagli.
    
    Se si vuole controllare al meglio l'azione di ThothAI è opportuno attivare Logfire. Il servizio è attivabile anche in locale, fatte le opportune attività di setup, per cui eventuali problematiche sulla privacy possono essere comunque superate. Per dettagli consultare la documentazione di Logfire. 


## 4 - Prossimi passi

Una volta completate queste attività preliminari, si può procedere con:

* [Installazione assistita su Docker](../1-docker_install/1.2-docker_assisted.md)
