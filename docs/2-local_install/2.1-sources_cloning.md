# Installazione di ThothAI - attività preliminari
**ThothAI** può essere installato sia come container `Docker`, per poter lavorare in un ambiente di produzione isolato e facilmente gestibile, sia in locale, per operare in un contesto di `'development'` che permetta agevolmente di realizzare e testare eventuali personalizzazioni.

Prerequisiti per lo sviluppo locale:

- `Python 3.13` o superiore
- `uv`, gestore di dipendenze per Python ([installazione](https://docs.astral.sh/uv/getting-started/){:target="_blank"})
- `Node.js v20+` / `npm` per frontend e servizi Node
- `Docker` per il servizio Qdrant (vector DB) anche quando ThothAI è avviato in locale

!!! note "Installazione dei prerequisiti"

    Le procedure di installazione di Python, uv, Node.js e Docker sono al di fuori dell'ambito di questo documento.
    Seguire i link sottostanti per accedere alle procedure di installazione:
    
    - [Python](https://www.python.org/){:target="_blank"}
    - [uv](https://docs.astral.sh/uv/getting-started/){:target="_blank"}
    - [Node.js](https://nodejs.org/){:target="_blank"}
    - [Docker](https://www.docker.com/){:target="_blank"}

## 1 - Attività preliminare - clonazione del repository

L'attività di installazione richiede prima di tutto l'acquisizione dei sorgenti del progetto. 
**ThothAI** è composto da un singolo repository che contiene sia il backend che il frontend.

Da linea di comando posizionarsi dove si preferisce e clonare il repository:

```bash
git clone https://github.com/mptyl/Thoth.git ThothAI
cd ThothAI
```

Dopo la clonazione, la struttura del progetto sarà simile a questa:

```
ThothAI/
├── backend/                # Applicazione Django (backend)
├── frontend/               # Applicazione Next.js (frontend)
├── docker/                 # File di configurazione Docker
├── scripts/                # Script di installazione e configurazione
├── config.yml              # Template di configurazione
├── .env.local.template     # Template delle variabili d'ambiente
└── ...
```

## 2 - Attività preliminare - configurazione di base

### 2.1 - Preparazione del file di configurazione {#preparazione-configurazione}

Per l'avvio in locale, il file di configurazione principale è `config.yml.local`, identico nella struttura a quello utilizzato per l'installazione Docker. Lo script `start-all.sh` legge questo file e genera automaticamente il file `.env.local` necessario per l'esecuzione.

**Primo passo obbligatorio**: creare il file di configurazione personale partendo dal template fornito:

```bash
cp config.yml config.yml.local
```

Il file `config.yml.local` contiene tutte le impostazioni necessarie per il funzionamento di ThothAI. Dovrete configurare:

- **Provider AI**: Attivare almeno un provider LLM, scegliendone uno o più tra OpenAI, Anthropic, Gemini, Mistral, DeepSeek, OpenRouter, Ollama e LMStudio
- **Servizio di embedding**: Configurare il servizio per l'embedding utilizzato da Qdrant e dalla gestione LSH (OpenAI, Mistral o Cohere)
- **Modello base di backend**: Scegliere il modello base per le attività gestite dal backend (descrizioni, documentazione, ERD, ecc.)
- **Database utilizzati**: Indicare i database che si intende utilizzare (SQLite, MariaDB, MySQL, PostgreSQL, Informix, SQL Server). Si possono utilizzare contemporaneamente più database.
- **Porte dei servizi**: Configurare le porte per i vari componenti se quelle di default sono in conflitto con altri servizi
- **Monitoraggio**: Opzionalmente configurare Logfire per il monitoraggio avanzato

!!! note "Provider LLM consigliato per iniziare"

    Per una rapida prova di ThothAI, si consiglia di utilizzare **OpenRouter** che fornisce accesso a centinaia di modelli LLM con una singola API key e include anche modelli gratuiti. La configurazione di default è già impostata per utilizzare OpenRouter. 

#### 2.1.1 - Struttura del file `config.yml.local`

Di seguito i parametri attualmente presenti nel file di configurazione `config.yml` alla radice del progetto. Usateli come riferimento durante la preparazione di `config.yml.local`:

```yaml
version: "1.0"

ai_providers:
  openai:
    enabled: false
    api_key: ""
  anthropic:
    enabled: false
    api_key: ""
  gemini:
    enabled: false
    api_key: ""
  mistral:
    enabled: false
    api_key: ""
  deepseek:
    enabled: false
    api_key: ""
    api_base: "https://api.deepseek.com/v1"
  openrouter:
    enabled: true
    api_key: ""
    api_base: "https://openrouter.ai/api/v1"
  ollama:
    enabled: false
    api_base: "http://127.0.0.1:11434"
  lm_studio:
    enabled: false
    api_base: "http://localhost:1234"
  groq:
    enabled: false
    api_key: ""

embedding:
  provider: "openai"
  api_key: ""
  model: "text-embedding-3-small"

backend_ai_model:
  ai_provider: "openrouter"
  ai_model: "google/gemini-2.5-flash"

databases:
  sqlite: true
  postgresql: true
  mysql: false
  mariadb: true
  sqlserver: true

ports:
  frontend: 3200
  backend: 8200
  sql_generator: 8180
  mermaid_service: 8003
  qdrant: 6334

monitoring:
  enabled: false
  logfire_token: ""

runtime:
  debug: true
  backend_log_level: "INFO"
  frontend_log_level: "INFO"

relevance_guard:
  strict_min_score: 0.75
  weak_min_score: 0.45
  drop_below: 0.30
  w_bm25: 0.60
  w_struct: 0.40
  use_rrf: false
  strict_fails_required: 2
```

#### 2.1.2 - Configurazione pratica di config.yml.local

Dopo aver copiato `config.yml` in `config.yml.local`, editare il file per inserire le proprie API keys e configurazioni.

**Configurazione minima per iniziare** (esempio con OpenRouter):

```yaml
ai_providers:
  openrouter:
    enabled: true
    api_key: "sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"  # Inserire la propria API key
    api_base: "https://openrouter.ai/api/v1"

embedding:
  provider: "openai"
  api_key: "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"  # Inserire la propria API key OpenAI
  model: "text-embedding-3-small"

backend_ai_model:
  ai_provider: "openrouter"
  ai_model: "google/gemini-2.5-flash"
```

**Le altre sezioni** (`databases`, `ports`, `monitoring`, `runtime`, `relevance_guard`) possono essere lasciate con i valori di default per iniziare.

!!! tip "Configurazione step-by-step"

    1. **Abilitare almeno un provider LLM**: Impostare `enabled: true` e inserire la propria `api_key`
    2. **Configurare l'embedding**: Inserire la chiave API del provider scelto (OpenAI, Cohere, o Mistral)
    3. **Verificare il backend AI model**: Assicurarsi che `ai_provider` e `ai_model` corrispondano a un provider abilitato
    4. **Opzionale**: Modificare le porte se quelle di default sono già in uso sul proprio sistema
    5. **Opzionale**: Configurare Logfire per il monitoraggio avanzato impostando `monitoring.enabled: true` e fornendo il token

!!! note "Generazione automatica di .env.local"

    **Non è necessario creare manualmente il file `.env.local`**. Lo script `start-all.sh` legge automaticamente `config.yml.local` e genera (o aggiorna) il file `.env.local` necessario per l'esecuzione locale. Se `config.yml.local` viene modificato, `.env.local` verrà rigenerato automaticamente al prossimo avvio quando si usa il comando `start_all.sh`.
    
    Il file `.env.local` generato contiene tutte le variabili d'ambiente necessarie per i servizi (chiavi API, porte, percorsi, ecc.) ed è automaticamente escluso dal version control tramite `.gitignore`.

## 3 - Attività preliminare - acquisizione delle API Keys

Prima di procedere con l'installazione vera e propria, è necessario dotarsi delle API Keys per:

### 3.1 - I servizi di generazione tramite LLM

**ThothAI** è predisposto per potersi collegare ai seguenti fornitori di API LLM:

* OpenAI 
* Gemini (di Google)
* Anthropic 
* Mistral
* DeepSeek
* LMStudio
* Ollama
* OpenRouter (che fa da proxy a centinaia di provider)

Ogni fornitore di API, a sua volta, permette di usare diversi modelli, che andranno specificati nel setup dell'applicazione.

In fase di configurazione occorre stabilire quali provider si desidera utilizzare. Per ognuno di essi è necessario procurarsi una API Key.
Se si utilizza Ollama o LMStudio in locale, ovviamente, non è necessaria alcuna API Key, ma solo indicare l'IP del server utilizzato se diverso da quello standard. 

Per procurarsi l'API Key occorre seguire le procedure previste dal provider.

!!! note "Assegnazione di una API Key"

    Le procedure di assegnazione di una API Key sono più o meno le stesse per ogni fornitore. 
    Creare delle API Key con il nome che preferite, come ad esempio "ThothKey" o simile, per ogni fornitore che si desidera utilizzare. IQueste chiavi devono essere indicate nel file di configurazione `config.yml` se si vuole utilizzare Docker, altrimenti nel file di configurazione .env.local

Se si usa OpenRouter, con una sola API Key si potrà accedere a una lunga lista di LLM, aventi dimensioni e costi molto variabili (alcuni modelli sono resi disponibili addirittura gratuitamente). Andare sul sito di [OpenRouter](https://openrouter.ai/) per ulteriori dettagli. 

Tramite `OpenRouter` si possono utilizzare modelli di provider non previsti dalla lista precedente, in quanto OpenRouter provvede a esporre sempre la stessa interfaccia, compatibile con l'interfaccia di OpenAI, utilizzabile per tutti i modelli che supporta.  

I valori di default impostati in ThothAI prevedono che tutti gli Agent che svolgono le varie fasi del processo usino un modello fornito da OpenRouter. 

L'utente può ovviamente cambiare questo setup (si vedrà più avanti come), ma, se si vuole provare rapidamente ThothAI, l'ideale è iniziare con l'acquisto di 5 o 10 dollari di servizi OpenRouter e lasciare le impostazioni di default che vengono impostate in fase di installazione. 

Dai nostri test, utilizzando la configurazione di default che prevede un largo uso di modelli economici nelle fasi meno critiche del processo, ogni esecuzione ha un costo variabile tra 2 e 5 centesimi di dollaro, in funzione del numero di agenti che si vuole attivare in parallelo (più agenti, più probabilità di avere un buon SQL, più costo).
Per cui, con una decina di dollari, si possono fare da 200 a 500 esecuzioni.

### 3.2 - Il servizio Logfire

[Logfire](https://pydantic.dev/logfire) è un servizio messo a disposizione da Pydantic per tracciare l'attività di applicazioni Python. Fino a 10 milioni di messaggi al mese è un servizio gratuito.  

ThothAI produce un log delle sue attività contemporaneamente su file, su console e su Logfire. Il log su Logfire persiste per circa 30 giorni e fornisce un elevato livello di dettaglio sulle operazioni registrate.

Su Logfire non vengono mai inviati dati sul contenuto dei database interrogati, ma, se non si vuole utilizzare il servizio, basta lasciare vuota la chiave e Logfire non verrà utilizzato. In questo caso rimarranno solo i file di log salvati in locale [(vedi logs)](../4-user_manual/4.3-logging/4.3.2-log_management.md). Non si avrà però lo stesso livello di granularità e di insight fornita da Logfire.

Se si vuole usare Logfire è necessario quindi andare su [Logfire](https://pydantic.dev/logfire) e crearsi una API Key. Scegliere un server europeo se si sta operando da un paese europeo per avere migliori performance. Salvare l'URL del server per le future consultazioni. 

!!! note "Il logging di un LLM"

    Loggare le attività effettuate da un LLM è un'attività piuttosto complessa. Con Logfire si può facilmente verificare il testo del prompt ricevuto dal LLM, l'eventuale attivazione di tool da parte dell'Agent in esecuzione e l'output generato, con abbondanza di dettagli.
    
    Se si vuole controllare al meglio l'azione di ThothAI è opportuno attivare Logfire. Il servizio è attivabile anche in locale, fatte le opportune attività di setup, per cui eventuali problematiche sulla privacy possono essere comunque superate. Per dettagli consultare la documentazione di Logfire. 

## 4 - Attività preliminare - verifica dei prerequisiti software

Prima di procedere con l'avvio dei servizi tramite `start-all.sh`, verificare che tutti i prerequisiti software siano correttamente installati:

### 4.1 - Verifica Python e uv

```bash
# Verifica versione Python (deve essere 3.13+)
python3 --version

# Verifica installazione uv
uv --version
```

Se `uv` non è installato, seguire le istruzioni su [https://docs.astral.sh/uv/getting-started/](https://docs.astral.sh/uv/getting-started/){:target="_blank"}

### 4.2 - Verifica Node.js e npm

```bash
# Verifica versione Node.js (deve essere v20+)
node --version

# Verifica npm
npm --version
```

### 4.3 - Verifica Docker

```bash
# Verifica Docker (necessario per Qdrant)
docker --version

# Verifica che Docker sia in esecuzione
docker ps
```

!!! warning "Docker è obbligatorio"

    Anche per l'installazione locale, Docker è necessario per eseguire il servizio Qdrant (vector database). Assicurarsi che Docker sia installato e in esecuzione prima di procedere con il lancio di `start-all.sh`.

## 5 - Prossimi passi

Una volta completate queste attività preliminari, si può procedere con:

* [Installazione in locale](./2.2-local.md)
* [Quickstart](../3-quickstart/3.1-quick_setup.md)
